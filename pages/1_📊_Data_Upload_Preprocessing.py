import streamlit as st
import pandas as pd
from pathlib import Path
import sys
import gc
import traceback
import io
from sklearn.preprocessing import RobustScaler
from styles import inject_global_styles, page_header  

inject_global_styles()  
page_header("ğŸ“Š Upload & Preprocessing", "Ã‰tape 1/6")  

# =========================
# PATHS & IMPORTS
# =========================
BASE_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(BASE_DIR / 'utils'))
sys.path.insert(0, str(BASE_DIR))

try:
    from utils import DataProcessor
    st.success("âœ… DataProcessor OK")
except Exception:
    st.error("âŒ utils/DataProcessor.py manquant")
    st.stop()

# =========================
# Ã‰TAT GLOBAL
# =========================
default_keys = {
    "df_loaded": False,
    "total_lines": 0,
    "processed_data": None,
    "raw_metrics": None,
    "df": None,
    "targets": None,
    "features": None,
    "prediction_mode": "multi",
    "source_type": None,
}

for k, v in default_keys.items():
    if k not in st.session_state:
        st.session_state[k] = v

# =========================
# ğŸ¯ OPTION A : DATASET DÃ‰MO OU UPLOAD
# =========================
st.subheader("ğŸ“ **Source des DonnÃ©es**")
data_source = st.radio(
    "Choisir la source",
    options=["ğŸ“‚ Upload mon fichier CSV", "ğŸ“ Utiliser dataset dÃ©mo (MS Teams 5G)"],
    index=1
)

# RESET si l'utilisateur change de source
if 'last_data_source' not in st.session_state:
    st.session_state.last_data_source = data_source
elif st.session_state.last_data_source != data_source:
    st.session_state.df_loaded = False
    st.session_state.df = None
    st.session_state.last_data_source = data_source

if data_source == "ğŸ“ Utiliser dataset dÃ©mo (MS Teams 5G)":
    demo_files = list(BASE_DIR.glob("*.csv"))
    if not demo_files:
        st.error("âŒ Aucun fichier CSV dÃ©mo trouvÃ© Ã  la racine !")
        st.info("ğŸ’¡ Place un fichier CSV (ex: MS_Teams.csv) dans le dossier racine.")
        st.stop()
    
    file_to_load = demo_files[0]
    st.success(f"âœ… **Dataset dÃ©mo** : {file_to_load.name} ({file_to_load.stat().st_size/1e6:.1f} MB)")
    st.session_state.source_type = "demo"
    st.session_state.file_to_load = file_to_load

else:
    uploaded_file = st.file_uploader(
        "ğŸ“¤ Upload ton fichier CSV 5G",
        type=["csv"],
        help="Colonnes attendues : Time, Length (minimum). Max 2GB."
    )
    
    if uploaded_file is None:
        st.warning("âš ï¸ Upload un fichier pour continuer")
        st.stop()
    
    st.session_state.uploaded_file_data = uploaded_file.getvalue()
    st.session_state.uploaded_file_name = uploaded_file.name
    st.session_state.source_type = "upload"
    st.success(f"âœ… **Fichier uploadÃ©** : {uploaded_file.name} ({len(st.session_state.uploaded_file_data)/1e6:.1f} MB)")

# =========================
# BOUTON CLEAR
# =========================
if st.button("ğŸ—‘ï¸ Clear Session", type="secondary"):
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.rerun()

# =========================
# Ã‰TAPE 1 : CHARGEMENT CSV
# =========================
if not st.session_state.df_loaded:
    if st.session_state.source_type == "demo":
        button_text = f"ğŸš€ **Ã‰TAPE 1 : Charger {st.session_state.file_to_load.name}**"
    else:
        button_text = f"ğŸš€ **Ã‰TAPE 1 : Charger {st.session_state.uploaded_file_name}**"
    
    if st.button(button_text, type="primary"):
        with st.spinner("ğŸ”„ Chargement par chunks..."):
            try:
                chunks = []
                total_lines = 0
                
                if st.session_state.source_type == "demo":
                    for chunk in pd.read_csv(st.session_state.file_to_load, chunksize=50000, low_memory=False):
                        if 'No.' in chunk.columns:
                            chunk['No.'] = pd.to_numeric(chunk['No.'], errors='coerce').astype('int32')
                        if 'Length' in chunk.columns:
                            chunk['Length'] = pd.to_numeric(chunk['Length'], errors='coerce').astype('float32')
                        chunks.append(chunk)
                        total_lines += len(chunk)
                        gc.collect()
                else:
                    file_like_object = io.BytesIO(st.session_state.uploaded_file_data)
                    for chunk in pd.read_csv(file_like_object, chunksize=50000, low_memory=False):
                        if 'No.' in chunk.columns:
                            chunk['No.'] = pd.to_numeric(chunk['No.'], errors='coerce').astype('int32')
                        if 'Length' in chunk.columns:
                            chunk['Length'] = pd.to_numeric(chunk['Length'], errors='coerce').astype('float32')
                        chunks.append(chunk)
                        total_lines += len(chunk)
                        gc.collect()
                
                df = pd.concat(chunks, ignore_index=True)
                del chunks
                gc.collect()

                st.session_state.df = df
                st.session_state.df_loaded = True
                st.session_state.total_lines = total_lines

                st.success(f"âœ… **{total_lines:,} lignes chargÃ©es**")
                st.rerun()

            except Exception as e:
                st.error(f"âŒ Erreur chargement : {e}")
                st.code(traceback.format_exc())
                st.stop()

# =========================
# DIAGNOSTIC : VÃ‰RIFICATION DES DONNÃ‰ES BRUTES
# =========================
if st.session_state.df_loaded:
    df = st.session_state.df
    
    st.info(f"ğŸ“Š **DataFrame prÃªt** : {len(df):,} lignes Ã— {len(df.columns)} colonnes")

    time_cols = [col for col in df.columns if 'time' in col.lower()]
    length_cols = [col for col in df.columns if 'length' in col.lower()]

    st.subheader("ğŸ‘ï¸ **AperÃ§u DonnÃ©es Brutes**")
    st.dataframe(df.head(5), use_container_width=True)

    # =========================
    # DIAGNOSTIC DES DONNÃ‰ES D'ENTRÃ‰E 
    # =========================
    st.subheader("ğŸ” **Diagnostic des DonnÃ©es d'EntrÃ©e**")

    if 'Length' in df.columns:
        st.write("**Colonne Length (brute) :**")
        length_stats = df['Length'].describe()
        st.write(length_stats)
    
        # ANALYSE INTELLIGENTE BASÃ‰E SUR LES VRAIES STATS
        length_min, length_max = df['Length'].min(), df['Length'].max()
        length_median = df['Length'].median()
    
        st.info(f"""
        **ğŸ“Š Analyse de la colonne Length :**
        â€¢ **Min :** {length_stats['min']:.0f} bytes
        â€¢ **Max :** {length_stats['max']:.0f} bytes  
        â€¢ **MÃ©diane :** {length_stats['50%']:.0f} bytes
        â€¢ **Moyenne :** {length_stats['mean']:.1f} bytes
    
        **âœ… InterprÃ©tation CORRECTE :**
        """)
    
        # LOGIQUE D'ANALYSE 
        analysis_points = []
    
        if length_min >= 0 and length_max <= 1500:
            analysis_points.append("âœ… **Tailles de paquets rÃ©seaux normales** (Ethernet MTU â‰ˆ 1500 bytes)")
    
        if length_min < 60:
            analysis_points.append("âš ï¸ **Certains paquets trÃ¨s petits** (< 60 bytes) - peut Ãªtre du trafic de contrÃ´le")
    
        if length_max > 9000:
            analysis_points.append("âš ï¸ **Paquets jumbo frames dÃ©tectÃ©s** (> 9000 bytes) - vÃ©rifier la configuration rÃ©seau")
    
        if length_min >= 0 and length_max <= 1 and length_max > 0.1:
            analysis_points.append("âŒ **PROBLÃˆME : DonnÃ©es dÃ©jÃ  normalisÃ©es** (valeurs 0-1)")
    
        if length_min < 0:
            analysis_points.append("âŒ **ERREUR : Valeurs nÃ©gatives** - donnÃ©es corrompues")
    
        # Afficher tous les points d'analyse
        for point in analysis_points:
            st.write(point)
    
        # CONCLUSION
        if "âœ… **Tailles de paquets rÃ©seaux normales**" in analysis_points:
            st.success("""
            **ğŸ¯ CONCLUSION : DonnÃ©es PARFAITES pour les mÃ©triques 5G !**
            Vos donnÃ©es `Length` sont en **bytes rÃ©els** â†’ vous pouvez activer 
            **"CrÃ©er mÃ©triques 5G dÃ©rivÃ©es"** sans problÃ¨me.
            """)
        elif "âŒ **PROBLÃˆME : DonnÃ©es dÃ©jÃ  normalisÃ©es**" in analysis_points:
            st.error("""
            **ğŸš¨ PROBLÃˆME DÃ‰TECTÃ‰ :** 
            Vos donnÃ©es `Length` sont dÃ©jÃ  normalisÃ©es (valeurs entre 0 et 1).
            **Action recommandÃ©e :** DÃ©sactivez "CrÃ©er mÃ©triques 5G dÃ©rivÃ©es".
            """)
    
        # AperÃ§u des valeurs
        st.write(f"**ğŸ” 5 premiÃ¨res valeurs Length :** {df['Length'].head().tolist()}")

    # =========================
    # CHOIX DE LA STRATÃ‰GIE
    # =========================
    st.markdown("---")
    st.subheader("âš™ï¸ **StratÃ©gie de PrÃ©paration**")
    
    col_strat1, col_strat2 = st.columns(2)
    
    with col_strat1:
        use_metrics = st.checkbox(
            "CrÃ©er mÃ©triques 5G dÃ©rivÃ©es",
            value=True,
            help="Ã€ DÃ‰SACTIVER si Length est dÃ©jÃ  normalisÃ©e (0-1)"
        )
    
    with col_strat2:
        force_raw = st.checkbox(
            "Utiliser donnÃ©es brutes (sans normalisation)",
            value=False,
            help="Pour garder les valeurs originales"
        )
    
    if not time_cols or not length_cols:
        st.warning("âš ï¸ Colonnes Time/Length manquantes â†’ MÃ©triques 5G indisponibles")
        use_metrics = False

    # =========================
    # BOUTON PRÃ‰PARATION
    # =========================
    if st.button("ğŸ¯ **Ã‰TAPE 2 : PrÃ©parer Dataset ML**", type="primary", key="prepare_ml"):
        try:
            with st.spinner("ğŸ”„ CrÃ©ation features + normalisation..."):
                gc.collect()

                # --- CAS 1 : MÃ‰TRIQUES 5G ---
                if use_metrics and time_cols and length_cols:
                    # 1. Extraire les colonnes utiles
                    useful_cols = [time_cols[0], length_cols[0]]
                    if 'Protocol' in df.columns:
                        useful_cols.append('Protocol')
                    
                    df_light = df[useful_cols].copy()
                    
                    # 2. VÃ‰RIFIER que Length n'est pas normalisÃ©e
                    length_values = df_light[length_cols[0]]
                    if length_values.between(0, 1).all() and length_values.max() > 0.1:
                        st.error("""
                        âŒ **ERREUR : Length est dÃ©jÃ  normalisÃ©e (0-1)**
                        
                        Les mÃ©triques calculÃ©es seront incorrectes.
                        **Action recommandÃ©e :**
                        1. DÃ©cochez "CrÃ©er mÃ©triques 5G dÃ©rivÃ©es"
                        2. Utilisez directement les colonnes numÃ©riques existantes
                        3. OU vÃ©rifiez votre fichier CSV source
                        """)
                        st.stop()
                    
                    processor = DataProcessor(df_light)
                    
                    # 3. Calculer les mÃ©triques RÃ‰ELLES
                    st.info("ğŸ“ˆ CrÃ©ation mÃ©triques 5G (freq='1S')...")
                    processor.create_traffic_metrics(
                        time_column=time_cols[0],
                        length_column=length_cols[0],
                        freq='1S'
                    )
                    
                    # 4. SAUVEGARDER les donnÃ©es BRUTES
                    raw_metrics_df = processor.get_processed_data()
                    st.session_state.raw_metrics = raw_metrics_df.copy()
                    
                    st.write("**âœ… MÃ©triques brutes calculÃ©es :**")
                    st.dataframe(raw_metrics_df.head(), use_container_width=True)
                    st.write(f"**Colonnes crÃ©Ã©es :** {raw_metrics_df.columns.tolist()}")
                    
                    # 5. NORMALISATION (sauf si dÃ©sactivÃ©e)
                    if not force_raw:
                        st.info("âœ… Normalisation (RobustScaler)...")
                        processor.normalize_data('robust')
                        normalized_df = processor.get_processed_data()
                        
                        st.write("**ğŸ“Š AprÃ¨s normalisation :**")
                        st.dataframe(normalized_df.head(), use_container_width=True)
                        
                        st.session_state.processed_data = normalized_df
                    else:
                        st.info("â­ï¸ Normalisation dÃ©sactivÃ©e")
                        st.session_state.processed_data = raw_metrics_df
                    
                    final_df = st.session_state.processed_data

                # --- CAS 2 : DONNÃ‰ES BRUTES (sans mÃ©triques) ---
                else:
                    st.info("â„¹ï¸ Utilisation colonnes numÃ©riques brutes")
                    
                    numeric_df = df.select_dtypes(include='number').copy()
                    numeric_df = numeric_df.fillna(method='ffill').fillna(0)
                    
                    st.write("**ğŸ“Š DonnÃ©es numÃ©riques brutes :**")
                    st.dataframe(numeric_df.head(), use_container_width=True)
                    
                    st.session_state.raw_metrics = numeric_df.copy()
                    
                    if not force_raw:
                        st.info("âœ… Normalisation (RobustScaler)...")
                        scaler = RobustScaler()
                        numeric_normalized = scaler.fit_transform(numeric_df)
                        numeric_normalized_df = pd.DataFrame(
                            numeric_normalized, 
                            columns=numeric_df.columns,
                            index=numeric_df.index
                        )
                        st.session_state.processed_data = numeric_normalized_df
                        
                        st.write("**ğŸ¯ AprÃ¨s normalisation :**")
                        st.dataframe(numeric_normalized_df.head(), use_container_width=True)
                    else:
                        st.session_state.processed_data = numeric_df
                    
                    final_df = st.session_state.processed_data
                
                # --- SÃ‰LECTION DES CIBLES (commun aux deux cas) ---
                st.subheader("ğŸ¯ **Choix des Cibles pour la PrÃ©diction**")
                
                final_numeric_cols = final_df.select_dtypes(include='number').columns.tolist()
                st.write(f"**Colonnes numÃ©riques disponibles ({len(final_numeric_cols)}) :**")
                st.json(final_numeric_cols)
                
                # DÃ©terminer les cibles par dÃ©faut intelligemment
                default_targets = []
                if "packet_count" in final_numeric_cols:
                    default_targets.append("packet_count")
                if "throughput_mbps" in final_numeric_cols:
                    default_targets.append("throughput_mbps")
                if len(default_targets) == 0 and len(final_numeric_cols) >= 2:
                    default_targets = final_numeric_cols[:2]
                
                selected_targets = st.multiselect(
                    "SÃ©lectionnez les colonnes Ã  prÃ©dire (cibles) :",
                    options=final_numeric_cols,
                    default=default_targets,
                    help="Choisissez au moins une cible. Multi-output possible.",
                    key="targets_selector_final"
                )
                
                if not selected_targets:
                    st.warning("âš ï¸ Veuillez sÃ©lectionner au moins une cible")
                    st.stop()
                
                # Calculer les features automatiquement
                feature_cols = [c for c in final_numeric_cols if c not in selected_targets]
                
                # Sauvegarder dans l'Ã©tat global
                st.session_state.targets = selected_targets
                st.session_state.features = feature_cols
                
                st.success(
                    f"ğŸ‰ **Configuration terminÃ©e !**\n"
                    f"â€¢ {len(final_df):,} Ã©chantillons\n"
                    f"â€¢ {len(feature_cols)} features â†’ {len(selected_targets)} cibles"
                )
                
                st.info(f"**ğŸ¯ Cibles :** {selected_targets}")
                st.info(f"**ğŸ”§ Features :** {feature_cols[:5]}{'...' if len(feature_cols) > 5 else ''}")
                
                # Afficher un aperÃ§u du dataset final
                st.subheader("ğŸ“‹ **AperÃ§u du Dataset Final**")
                st.dataframe(final_df.head(), use_container_width=True)
                
                # Message de navigation
                st.success("âœ… **Dataset prÃªt ! Page 2 â†’ EntraÃ®ner modÃ¨les**")
                
                if st.button("â¡ï¸ Aller Ã  la Page 2 - EntraÃ®nement", type="secondary"):
                    st.switch_page("pages/2_ğŸ§ _Model_Training.py")

        except Exception as e:
            st.error(f"âŒ Erreur prÃ©paration : {str(e)}")
            st.code(traceback.format_exc())

# =========================
# COMPARAISON : BRUTES vs NORMALISÃ‰ES (si disponible)
# =========================
if st.session_state.get('raw_metrics') is not None and st.session_state.get('processed_data') is not None:
    st.markdown("---")
    st.subheader("ğŸ” **Comparaison : DonnÃ©es Brutes vs NormalisÃ©es**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“Š DonnÃ©es Brutes (Valeurs Physiques)**")
        st.dataframe(st.session_state.raw_metrics.head(), use_container_width=True)
        st.caption("UnitÃ©s rÃ©elles : packets/s, Mbps, bytes, etc.")
    
    with col2:
        st.markdown("**ğŸ¯ DonnÃ©es NormalisÃ©es (Pour ML)**")
        st.dataframe(st.session_state.processed_data.head(), use_container_width=True)
        st.caption("CentrÃ©es-rÃ©duites (RobustScaler) - meilleure convergence")
    
    # Exemple de conversion pour une mÃ©trique courante
    common_cols = set(st.session_state.raw_metrics.columns) & set(st.session_state.processed_data.columns)
    if common_cols:
        sample_col = list(common_cols)[0]
        if sample_col in st.session_state.raw_metrics.columns and sample_col in st.session_state.processed_data.columns:
            raw_sample = st.session_state.raw_metrics[sample_col].iloc[0]
            norm_sample = st.session_state.processed_data[sample_col].iloc[0]
            
            st.info(f"""
            **Exemple de transformation pour '{sample_col}' :**
            - **Valeur brute** : `{raw_sample:.6f}` (unitÃ© physique)
            - **Valeur normalisÃ©e** : `{norm_sample:.6f}` (sans unitÃ©, Ã©chelle standard)
            """)

# =========================
# AFFICHAGE FINAL (si dÃ©jÃ  prÃ©parÃ©)
# =========================
if st.session_state.get('processed_data') is not None:
    st.markdown("---")
    st.subheader("ğŸ“ˆ **RÃ©sumÃ© Dataset ML (DÃ©jÃ  prÃ©parÃ©)**")

    processed_df = st.session_state.processed_data

    col1, col2 = st.columns(2)
    with col1:
        st.metric("ğŸ“Š Ã‰chantillons", f"{len(processed_df):,}")
        st.metric("ğŸ“ Colonnes totales", len(processed_df.columns))

    with col2:
        if 'timestamp' in processed_df.columns or processed_df.index.name == 'Time':
            st.metric("â±ï¸ FrÃ©quence", "1 seconde")
        if st.session_state.get('targets'):
            st.metric("ğŸ¯ Cibles", len(st.session_state.targets))
    
    if st.session_state.get('targets'):
        st.info(f"**ğŸ¯ Cibles configurÃ©es :** {st.session_state.targets}")
        if st.session_state.get('features'):
            st.info(f"**ğŸ”§ Nombre de features :** {len(st.session_state.features)}")
    
    st.success("âœ… **Dataset dÃ©jÃ  prÃ©parÃ©. Vous pouvez passer Ã  la page 2.**")